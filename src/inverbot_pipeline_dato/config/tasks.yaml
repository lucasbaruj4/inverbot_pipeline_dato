extract_task:
  description: >
    Extract metadata and structured data from financial and economic web sources using the provided scraping tools. 
    Each tool is specialized for a specific source and will extract data according to predefined JSON schemas.
    
    Process:
    1. Use the appropriate tool for each data source (BVA, INE, DNIT, etc.)
    2. For each tool, the data will be extracted according to its specific schema:
       - Informes/documentos (Informe_General): títulos, URLs, fechas
       - Emisores/entidades (Emisores): nombres, categorías
       - En algunos casos, datos estadísticos o macroeconómicos cuando están visibles en imágenes o tablas
    
    Important considerations:
    - Initial extractions should be in test_mode=True to validate functionality with minimal credit usage
    - Focus on metadata extraction only - do not attempt to parse document contents at this stage
    - Maintain relationships between extracted entities using consistent IDs
    - Be aware that some data is presented in images or interactive elements (especially in DNIT inversión)
    - For pages with pagination, ensure all relevant pages are explored
    
    Your extraction results will be passed to the 'processor' agent for structured database storage and to the 
    'vector' agent for document vectorization.
  expected_output: >
    JSON object with the following structure:
    {
      "source_name": {
        "Informe_General": [
          {
            "id_informe": integer,
            "titulo_informe": string,
            "url_descarga_original": string,
            "fecha_publicacion": string (YYYY-MM-DD format when available),
            "id_emisor": integer (reference to Emisores table)
          },
          ...
        ],
        "Emisores": [
          {
            "id_emisor": integer,
            "nombre_emisor": string
          },
          ...
        ],
        "urls_pendientes": [
          string (URLs that need further processing)
        ]
      },
      ...
    }
  agent: extractor
  output_file: 'output/try_1/raw_extraction_output.txt'

process_task:
  description: >
    Read the extracted data from the output of the previous task, the file name is 'raw_extraction_output.txt'. Map to structured database schemas like Resumen_Informe_Financiero. Handle financial, movements, macro, contracts. Return structured data and save to output file.
  expected_output: >
    Structured dicts for schemas like Resumen_Informe_Financiero
  agent: processor
  # context: extract_task
  output_file: 'output/try_1/structured_data_output.txt'

vectorize_task: 
  description: >
    Read both 'output/try_1/raw_extraction_output.txt' and 'output/try_1/structured_data_output.txt' to identify content relationships between raw and structured data, then create semantic embeddings for comprehensive search capabilities. Chunk the raw content into overlapping 300-800 token segments while preserving semantic boundaries, ensuring each chunk maps back to relevant structured data entries. Generate high-quality embeddings using appropriate models and prepare Pinecone-ready vectors with unique IDs, normalized embedding values, and rich metadata including source references, structured data relationships, content classification, processing timestamps, and semantic density scores. Save the complete vector dataset to an output file, ensuring the vectorized content complements structured data by capturing detailed context and nuances that summaries compress, enabling both precise structured queries and semantic similarity search across the full content spectrum.
  expected_output: >
    List of dicts: [{'id': str, 'values': [float], 'metadata': {'source_file': str, 'structured_data_refs': [str], 'content_type': str, 'chunk_index': int, 'word_count': int}}]
  # context: [process_task, extract_task]
  output_file: 'output/try_1/vector_data_output.txt'
  agent: vector

load_task:
  description: >
    Read the structured data from 'output/try_1/structured_data_output.txt' and vector data from 'output/try_1/vector_data_output.txt' and print them alongside eachother so that a Human can read which vector data refers to which structured data.
  expected_output: >
    Dict with "{'id_of_structured_data': 'data_of_correspondent_vector'}"
  output_file: 'output/try_1/loading_results_output.txt'
  agent: loader
  # context: [process_task, vectorize_task]
