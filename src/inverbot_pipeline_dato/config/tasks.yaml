extract_task:
  description: >
    Comprehensive raw content extraction from all 10 Paraguayan financial data sources using specialized scraping tools.
    Each tool captures complete, unstructured content for intelligent processing by downstream agents.
    
    NEW ARCHITECTURE: Raw Content Extraction Strategy
    Instead of complex structured extraction, focus on comprehensive content capture:
    - Gather ALL page text, headings, and content without structuring
    - Capture ALL links and document URLs for comprehensive coverage
    - Extract metadata about page structure and navigation elements
    - Let the Processor Agent handle intelligent data structuring
    
    Tool Usage Requirements - MANDATORY: ALL 10 TOOLS MUST BE USED:
    ðŸš¨ CRITICAL: You MUST call ALL 10 scraper tools. This is non-negotiable. Do NOT stop after using just a few tools. 
    ðŸš¨ FAILURE TO USE ALL 10 TOOLS WILL RESULT IN INCOMPLETE DATA.
    Call them in this order:
    
    1. **BVA Sources** (4 tools) - MUST USE ALL:
       - scrape_bva_emisores: Raw content from company listings and entity pages
       - scrape_bva_daily: Raw content from daily market reports and data
       - scrape_bva_monthly: Raw content from monthly summaries and volumes
       - scrape_bva_annual: Raw content from annual report pages
    
    2. **Government Data** (3 tools) - MUST USE ALL:
       - scrape_datos_gov: Raw content from open government data portal
       - scrape_ine_main: Raw content from national statistics institute
       - scrape_ine_social: Raw content from social statistics publications
    
    3. **Contracts & Investment** (3 tools) - MUST USE ALL:
       - scrape_contrataciones: Raw content from public procurement portal
       - scrape_dnit_investment: Raw content from investment information pages
       - scrape_dnit_financial: Raw content from financial regulatory reports
    
    ðŸ”¥ ABSOLUTELY CRITICAL: You must call ALL 10 tools, even if some fail or return empty data. 
    ðŸ”¥ DO NOT STOP EARLY - Continue until all 10 tools have been executed!
    ðŸ”¥ The output MUST contain content from all 10 sources, even if some are errors!
    
    Content Capture Strategy:
    - Begin with test_mode=True for validation (limited credit usage)
    - Prioritize comprehensive content gathering over data structuring
    - Capture complete page text including navigation, forms, and tables
    - Collect all links especially PDF and document download links
    - Gather page metadata for source identification and processing hints
    - Handle pagination and dynamic content to maximize content coverage
    
    Quality Requirements:
    - Ensure complete content capture without truncation
    - Validate that all links and documents are properly extracted
    - Report any scraping errors or access limitations
    - Provide rich metadata for intelligent downstream processing
    
    Output provides raw content for Processor Agent to intelligently structure into database format.
  expected_output: >
    ðŸš¨ MANDATORY OUTPUT VALIDATION: You MUST call ALL 10 tools and populate ALL 10 content fields.
    NO FIELDS CAN BE NULL - if a tool fails, use fallback mock data.
    
    VALIDATION CHECKLIST (all must be populated):
    âœ… scrape_bva_emisores â†’ bva_sources.emisores_content
    âœ… scrape_bva_daily â†’ bva_sources.daily_content  
    âœ… scrape_bva_monthly â†’ bva_sources.monthly_content
    âœ… scrape_bva_annual â†’ bva_sources.annual_content
    âœ… scrape_datos_gov â†’ government_sources.datos_gov_content
    âœ… scrape_ine_main â†’ government_sources.ine_main_content
    âœ… scrape_ine_social â†’ government_sources.ine_social_content
    âœ… scrape_contrataciones â†’ contracts_investment_sources.contracts_content
    âœ… scrape_dnit_investment â†’ contracts_investment_sources.dnit_investment_content
    âœ… scrape_dnit_financial â†’ contracts_investment_sources.dnit_financial_content
    
    Raw content structure organized by data source with comprehensive content capture:
    {
      "bva_sources": {
        "emisores_content": {"page_content": str, "links": [str], "documents": [str], "metadata": dict},
        "daily_content": {"page_content": str, "links": [str], "documents": [str], "metadata": dict},
        "monthly_content": {"page_content": str, "links": [str], "documents": [str], "metadata": dict},
        "annual_content": {"page_content": str, "links": [str], "documents": [str], "metadata": dict}
      },
      "government_sources": {
        "datos_gov_content": {"page_content": str, "links": [str], "documents": [str], "metadata": dict},
        "ine_main_content": {"page_content": str, "links": [str], "documents": [str], "metadata": dict},
        "ine_social_content": {"page_content": str, "links": [str], "documents": [str], "metadata": dict}
      },
      "contracts_investment_sources": {
        "contracts_content": {"page_content": str, "links": [str], "documents": [str], "metadata": dict},
        "dnit_investment_content": {"page_content": str, "links": [str], "documents": [str], "metadata": dict},
        "dnit_financial_content": {"page_content": str, "links": [str], "documents": [str], "metadata": dict}
      },
      "extraction_summary": {
        "total_sources_processed": int, "content_length_total": int, "links_found_total": int, "documents_found_total": int
      }
    }

  output_file: 'output/try_1/raw_extraction_output.txt'

process_task:
  description: >
    UPDATED PIPELINE ARCHITECTURE: Process raw extracted content through a streamlined 4-stage pipeline 
    using new file-based tools to create production-ready structured data for all 14 Supabase tables.
    
    CRITICAL: Use the NEW PIPELINE-INTEGRATED TOOLS - all tools now read from files automatically.
    
    Sequential Tool Workflow:
    
    1. **Extract Structured Data from Raw Content** - Intelligent Content Structuring
       - TOOL: extract_structured_data_from_raw (NEW: reads from raw_extraction_output.txt automatically)
       - Reads raw content from extractor agent's output file
       - Intelligently extracts structured data from unstructured text
       - Processes BVA financial data (bond emissions, company ratings, trading volumes)
       - Routes processing based on source (BVA, INE, DNIT, contracts, etc.)
       - Converts raw content into structured format for all 14 database tables
       - Handles malformed JSON and incomplete data gracefully
       - Output: Writes structured_data_output.txt with organized data
    
    2. **Normalize Data Tool** - Data Cleaning & Standardization
       - TOOL: normalize_data (NEW: reads from structured_data_output.txt automatically)
       - Reads structured data from step 1 output file
       - Cleans HTML artifacts, special characters, encoding issues
       - Standardizes date formats (YYYY-MM-DD), number formats (decimal)
       - Normalizes emisor names to lowercase with dashes
       - Handles missing values and data type inconsistencies
       - Output: Writes normalized_data_output.txt with cleaned data
    
    3. **Validate Data Tool** - Schema Validation  
       - TOOL: validate_data (NEW: reads from normalized_data_output.txt automatically)
       - Reads normalized data from step 2 output file
       - Validates against all 14 Supabase table schemas (UPDATED schemas)
       - Checks data types, field lengths, required constraints
       - Validates foreign key references and relationships
       - Separates valid data from invalid entries with error reporting
       - Output: Writes validated_data_output.txt with validation results
    
    4. **Create Entity Relationships Tool** - Relationship Building
       - TOOL: create_entity_relationships (NEW: reads from validated_data_output.txt automatically)
       - Reads validated data from step 3 output file
       - Resolves entity names to proper IDs (emisor names â†’ id_emisor)
       - Creates master entities first (Categoria_Emisor, Moneda, etc.)
       - Establishes foreign key relationships across all tables
       - Handles missing references with appropriate defaults
       - Output: Writes relationships_data_output.txt with complete entity relationships
    
    Quality Checkpoints:
    - Validate data integrity at each stage
    - Report processing statistics and error counts
    - Ensure referential integrity across all relationships
    - Confirm all required fields are populated
    
    Target Schemas: All 14 Supabase tables including Categoria_Emisor, Emisores, Moneda, Frecuencia, 
    Tipo_Informe, Periodo_Informe, Unidad_Medida, Instrumento, Informe_General, 
    Resumen_Informe_Financiero, Dato_Macroeconomico, Movimiento_Diario_Bolsa, Licitacion_Contrato.
  expected_output: >
    File write confirmation with processing summary. The structured data has been written to output/try_1/structured_data_output.txt.
    Processing Summary:
    {
      "file_write_status": {
        "status": "success",
        "file_path": "output/try_1/structured_data_output.txt",
        "total_records_written": int,
        "tables_written": [str],
        "file_size_kb": float
      },
      "processing_summary": {
        "tables_populated": [str],
        "total_records_processed": int,
        "documents_extracted": int,
        "relationships_created": int,
        "duplicates_filtered": int,
        "validation_errors": int,
        "data_quality_score": float
      },
      "key_statistics": {
        "emisores_created": int,
        "informes_processed": int,
        "macroeconomic_data_points": int,
        "contracts_identified": int,
        "daily_movements": int
      }
    }

  context: [extract_task]
  output_file: 'output/try_1/structured_data_output.txt'

vectorize_task: 
  description: >
    UPDATED PIPELINE ARCHITECTURE: Transform structured financial data into high-quality vector embeddings 
    for semantic search across 3 Pinecone indices using new pipeline-integrated tools.
    
    CRITICAL: Use the NEW PIPELINE-INTEGRATED TOOL - automatically reads from structured data files.
    
    Sequential Tool Workflow:
    
    1. **Process Structured Data for Vectorization** - Complete Vector Generation Pipeline
       - TOOL: process_structured_data_for_vectorization (NEW: reads from relationships_data_output.txt automatically)
       - Reads structured data with relationships from processor agent output
       - Creates meaningful vector content for all 3 Pinecone indices:
         * documento_informe_vector: Bond operations, issuer information, financial movements
         * dato_macroeconomico_vector: Economic indicators with context (trading volumes, economic metrics)
         * licitacion_contrato_vector: Contract and tender information
       - Generates descriptive text from structured financial data
       - Creates proper metadata linking to Supabase records
       - Assigns unique UUIDs for each vector
       - Handles all data transformation internally
       - Output: Writes vector_data_output.txt with complete vector-ready data
    
    2. **Write Vector Data to File** - File Persistence [AUTOMATIC]
       - TOOL: write_vector_data_to_file (if needed for additional processing)
       - Handles large vector datasets that exceed LLM response limits
       - Ensures data persistence for downstream loading agent
       - Already integrated into process_structured_data_for_vectorization
       - Output: Confirms vector data file creation
    
    Index-Specific Processing:
    - **documentos-informes-vector**: Complete document content with structured data references
    - **dato-macroeconomico-vector**: Economic context with metric relationships  
    - **licitacion-contrato-vector**: Contract details with entity associations
    
    Content Sources:
    - Extracted document text from processor agent output (structured_data_output.txt)
    - Structured data records from all 14 Supabase tables
    - Economic indicator descriptions and context
    - Contract and tender detailed information
    
    Quality Requirements:
    - Maintain semantic coherence across chunks
    - Preserve document structure and relationships
    - Ensure proper metadata for each vector index
    - Generate embeddings compatible with Gemini (768 dimensions)
    - Link vectors to corresponding Supabase structured data
  expected_output: >
    File write confirmation with vectorization summary. The vector data has been written to output/try_1/vector_data_output.txt.
    Vectorization Summary:
    {
      "file_write_status": {
        "status": "success",
        "file_path": "output/try_1/vector_data_output.txt",
        "total_vectors_written": int,
        "indices_written": [str],
        "file_size_kb": float
      },
      "vectorization_statistics": {
        "total_chunks_created": int,
        "pdf_documents_processed": int,
        "excel_documents_processed": int,
        "duplicates_filtered": int,
        "indices_populated": [str]
      },
      "index_breakdown": {
        "documentos-informes-vector": {
          "vectors_created": int,
          "average_chunk_size": int
        },
        "dato-macroeconomico-vector": {
          "vectors_created": int,
          "economic_indicators": int
        },
        "licitacion-contrato-vector": {
          "vectors_created": int,
          "contracts_processed": int
        }
      }
    }
  context: [process_task, extract_task]
  output_file: 'output/try_1/vector_data_output.txt'


load_task:
  description: >
    ðŸš¨ MANDATORY: You MUST call BOTH loading tools in sequence - Supabase AND Pinecone loading.
    BOTH tools are required for complete pipeline success. DO NOT SKIP ANY TOOL.
    
    UPDATED PIPELINE ARCHITECTURE: Execute database loading operations for both structured data (Supabase) 
    and vector data (Pinecone) using new pipeline-integrated tools that automatically read from files.
    
    CRITICAL: Use the NEW PIPELINE-INTEGRATED TOOLS - all tools now read from files automatically.
    
    MANDATORY Sequential Tool Workflow (BOTH TOOLS REQUIRED):
    
    **PRIMARY OPTION - Use This First:**
    0. **Complete Pipeline Loading - Both Supabase and Pinecone** 
       - TOOL: complete_pipeline_loading (RECOMMENDED: Forces both operations)
       - Automatically executes both Supabase AND Pinecone loading
       - Guarantees both loading reports are created
       - Use this tool to ensure 100% pipeline completion
    
    **ALTERNATIVE - Individual Tools (if needed):**
    1. **Load Structured Data to Supabase from Pipeline** - Complete Database Loading
       - TOOL: load_structured_data_to_supabase_pipeline (NEW: reads from relationships_data_output.txt automatically)
       - Reads structured data with relationships from processor agent output
       - Loads data to all 14 Supabase tables with proper sequence:
         * Master tables: Categoria_Emisor, Moneda, Frecuencia, Unidad_Medida, etc.
         * Entity tables: Emisores, Instrumento, Tipo_Informe, etc.
         * Transaction tables: Movimiento_Diario_Bolsa, Dato_Macroeconomico, etc.
       - Handles foreign key constraints and relationships automatically
       - Uses corrected schema definitions (nombre_instrumento not simbolo_instrumento)
       - Generates deterministic IDs from unique field combinations
       - In TEST_MODE: Saves data as JSON files instead of database loading
       - Output: Writes supabase_loading_report.json with comprehensive results
    
    2. **Load Vectors to Pinecone from Pipeline** - Complete Vector Loading
       - TOOL: load_vectors_to_pinecone_pipeline (NEW: reads from vector_data_output.txt automatically)
       - Reads vector data from vectorization agent output
       - Loads vectors to 3 Pinecone indices:
         * documento_informe_vector: Financial operations and issuer data
         * dato_macroeconomico_vector: Economic indicators and trading volumes
         * licitacion_contrato_vector: Contract and tender information
       - In TEST_MODE: Generates mock 768-dimensional embeddings deterministically
       - Creates proper metadata for semantic search
       - Handles batch processing automatically
       - Output: Writes pinecone_loading_report.json with comprehensive results
    
    3. **Check Loading Status** - Status Verification (if needed)
       - TOOL: check_loading_status (for additional verification if needed)
       - Verifies loading results from both tools
       - Checks file outputs and data integrity
       - Generates comprehensive ETL pipeline status report
       - Output: Complete pipeline status verification
    
    Database Operations:
    - **Supabase**: 14 tables with proper constraint handling
    - **Pinecone**: 3 indices with 768-dimension Gemini embeddings
    - **Batch Processing**: Optimized for performance and reliability
    - **Error Handling**: Graceful failure recovery and detailed reporting
    
    Quality Assurance:
    - Pre-loading validation prevents data corruption
    - Proper loading sequence maintains referential integrity
    - Comprehensive status checks ensure data quality
    - Detailed reporting enables troubleshooting
    
    Production Requirements:
    - All environment variables must be configured
    - Database connections must be established
    - API keys for Supabase, Pinecone, and Gemini required
    - Error handling for network issues and API limits
  expected_output: >
    ðŸš¨ MANDATORY VALIDATION: You MUST call both loading tools and generate BOTH reports:
    âœ… load_structured_data_to_supabase_pipeline â†’ supabase_loading_report.json
    âœ… load_vectors_to_pinecone_pipeline â†’ pinecone_loading_report.json
    
    BOTH files must be created - no exceptions!
    
    Comprehensive ETL pipeline completion report:
    {
      "loading_results": {
        "supabase_results": {
          "tables_loaded": [str],
          "total_records_inserted": int,
          "loading_errors": [str],
          "batch_statistics": {"table_name": {"batches": int, "records": int}},
          "foreign_key_validations": {"table_name": "status"}
        },
        "pinecone_results": {
          "indices_loaded": [str], 
          "total_vectors_inserted": int,
          "embedding_errors": [str],
          "batch_statistics": {"index_name": {"batches": int, "vectors": int}},
          "embedding_generation_time": float
        }
      },
      "data_integrity_report": {
        "supabase_validation": {
          "record_counts": {"table_name": int},
          "constraint_violations": [str],
          "orphaned_records": [str]
        },
        "pinecone_validation": {
          "vector_counts": {"index_name": int},
          "embedding_dimension_check": "768_confirmed",
          "metadata_completeness": {"index_name": float}
        },
        "cross_database_links": {
          "structured_to_vector_mapping": int,
          "unmapped_records": [str]
        }
      },
      "pipeline_summary": {
        "total_execution_time": float,
        "extraction_records": int,
        "processing_success_rate": float,
        "vectorization_success_rate": float,
        "loading_success_rate": float,
        "overall_pipeline_status": "SUCCESS|PARTIAL|FAILED"
      }
    }

  context: [process_task, vectorize_task]
  output_file: 'output/try_1/loading_results_output.txt'
