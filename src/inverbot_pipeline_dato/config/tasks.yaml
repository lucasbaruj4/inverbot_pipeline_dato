extract_task:
  description: >
    Comprehensive raw content extraction from all 10 Paraguayan financial data sources using specialized scraping tools.
    Each tool captures complete, unstructured content for intelligent processing by downstream agents.
    
    NEW ARCHITECTURE: Raw Content Extraction Strategy
    Instead of complex structured extraction, focus on comprehensive content capture:
    - Gather ALL page text, headings, and content without structuring
    - Capture ALL links and document URLs for comprehensive coverage
    - Extract metadata about page structure and navigation elements
    - Let the Processor Agent handle intelligent data structuring
    
    Tool Usage Requirements - MUST USE ALL 10 TOOLS:
    You MUST call ALL 10 scraper tools. Do NOT stop after using just a few. Call them in this order:
    
    1. **BVA Sources** (4 tools) - MUST USE ALL:
       - scrape_bva_emisores: Raw content from company listings and entity pages
       - scrape_bva_daily: Raw content from daily market reports and data
       - scrape_bva_monthly: Raw content from monthly summaries and volumes
       - scrape_bva_annual: Raw content from annual report pages
    
    2. **Government Data** (3 tools) - MUST USE ALL:
       - scrape_datos_gov: Raw content from open government data portal
       - scrape_ine_main: Raw content from national statistics institute
       - scrape_ine_social: Raw content from social statistics publications
    
    3. **Contracts & Investment** (3 tools) - MUST USE ALL:
       - scrape_contrataciones: Raw content from public procurement portal
       - scrape_dnit_investment: Raw content from investment information pages
       - scrape_dnit_financial: Raw content from financial regulatory reports
    
    IMPORTANT: You must call ALL 10 tools, even if some fail or return empty data. Do not stop early!
    
    Content Capture Strategy:
    - Begin with test_mode=True for validation (limited credit usage)
    - Prioritize comprehensive content gathering over data structuring
    - Capture complete page text including navigation, forms, and tables
    - Collect all links especially PDF and document download links
    - Gather page metadata for source identification and processing hints
    - Handle pagination and dynamic content to maximize content coverage
    
    Quality Requirements:
    - Ensure complete content capture without truncation
    - Validate that all links and documents are properly extracted
    - Report any scraping errors or access limitations
    - Provide rich metadata for intelligent downstream processing
    
    Output provides raw content for Processor Agent to intelligently structure into database format.
  expected_output: >
    IMPORTANT: You MUST populate ALL 10 content fields below by calling ALL 10 scraper tools.
    Raw content structure organized by data source with comprehensive content capture:
    {
      "bva_sources": {
        "emisores_content": {"page_content": str, "links": [str], "documents": [str], "metadata": dict},
        "daily_content": {"page_content": str, "links": [str], "documents": [str], "metadata": dict},
        "monthly_content": {"page_content": str, "links": [str], "documents": [str], "metadata": dict},
        "annual_content": {"page_content": str, "links": [str], "documents": [str], "metadata": dict}
      },
      "government_sources": {
        "datos_gov_content": {"page_content": str, "links": [str], "documents": [str], "metadata": dict},
        "ine_main_content": {"page_content": str, "links": [str], "documents": [str], "metadata": dict},
        "ine_social_content": {"page_content": str, "links": [str], "documents": [str], "metadata": dict}
      },
      "contracts_investment_sources": {
        "contracts_content": {"page_content": str, "links": [str], "documents": [str], "metadata": dict},
        "dnit_investment_content": {"page_content": str, "links": [str], "documents": [str], "metadata": dict},
        "dnit_financial_content": {"page_content": str, "links": [str], "documents": [str], "metadata": dict}
      },
      "extraction_summary": {
        "total_sources_processed": int, "content_length_total": int, "links_found_total": int, "documents_found_total": int
      }
    }

  output_file: 'output/try_1/raw_extraction_output.txt'

process_task:
  description: >
    Process raw extracted content through a comprehensive 6-stage pipeline to create production-ready structured data
    for all 14 Supabase tables. NEW ARCHITECTURE: First stage intelligently converts raw content to structured format.
    
    Sequential Tool Workflow:
    
    1. **extract_text_from_pdf** & **extract_text_from_excel** - Document Content Extraction [LIMITED MODE]
       - IMPORTANT: Document limits are active - Max 3 PDFs and 2 Excel files
       - First read raw_extraction_output.txt to get PDF and Excel URLs
       - Process PDFs until limit is reached (max 3)
       - Process Excel files until limit is reached (max 2)
       - Tools will automatically skip documents once limits are reached
       - Skipped documents will be processed later via direct_processor.py
       - Output: Extracted document text content for structured processing
    
    2. **extract_structured_data_from_raw** - Intelligent Content Structuring
       - Input: Raw content from extractor + extracted document text from step 1
       - Analyze content type based on source URL and content patterns
       - Intelligently extract structured data from unstructured text
       - Route processing based on source (BVA, INE, DNIT, contracts, etc.)
       - Convert raw content into preliminary database record format
       - Output: Structured data organized by 14 database tables
    
    3. **normalize_data** - Data Cleaning & Standardization
       - Input: Structured data from step 2
       - Clean HTML artifacts, special characters, encoding issues
       - Standardize date formats (YYYY-MM-DD), number formats (decimal)
       - Convert string numbers to proper numeric types
       - Handle missing values and data type inconsistencies
       - Output: Clean, normalized data dictionary
    
    4. **validate_data** - Schema Validation
       - Input: Normalized data from step 3
       - Validate against all 14 Supabase table schemas
       - Check data types, field lengths, required constraints
       - Validate foreign key references and relationships
       - Separate valid data from invalid entries with error reporting
       - Output: Validated data + validation error report
    
    5. **create_entity_relationships** - Relationship Building
       - Input: Validated data from step 4
       - Resolve entity names to proper IDs (emisor names â†’ id_emisor)
       - Create master entities first (Categoria_Emisor, Moneda, etc.)
       - Establish foreign key relationships across all tables
       - Handle missing references with appropriate defaults
       - Output: Data with complete entity relationships
    
    6. **structure_extracted_data** - Final Organization
       - Input: Data with relationships from step 5
       - Organize data by target Supabase tables
       - Apply loading priorities (master tables first)
       - Create optimized batches for bulk loading
       - Add loading metadata and recommendations
       - Output: Production-ready structured data
    
    7. **filter_duplicate_data** - Duplicate Prevention
       - Input: Structured data from step 6
       - Check existing Supabase data to prevent duplicates
       - Use table-specific unique field combinations
       - Filter out existing records, keep only new data
       - Output: Final deduplicated data ready for loading
    
    8. **write_structured_data_to_file** - File Persistence [CRITICAL - MUST BE LAST]
       - Input: Final structured data from step 7
       - Write complete structured data to output/try_1/structured_data_output.txt
       - Handles large datasets that exceed LLM response limits
       - Ensures data persistence for downstream agents
       - Output: Success status with file path and statistics
    
    Quality Checkpoints:
    - Validate data integrity at each stage
    - Report processing statistics and error counts
    - Ensure referential integrity across all relationships
    - Confirm all required fields are populated
    
    Target Schemas: All 14 Supabase tables including Categoria_Emisor, Emisores, Moneda, Frecuencia, 
    Tipo_Informe, Periodo_Informe, Unidad_Medida, Instrumento, Informe_General, 
    Resumen_Informe_Financiero, Dato_Macroeconomico, Movimiento_Diario_Bolsa, Licitacion_Contrato.
  expected_output: >
    File write confirmation with processing summary. The structured data has been written to output/try_1/structured_data_output.txt.
    Processing Summary:
    {
      "file_write_status": {
        "status": "success",
        "file_path": "output/try_1/structured_data_output.txt",
        "total_records_written": int,
        "tables_written": [str],
        "file_size_kb": float
      },
      "processing_summary": {
        "tables_populated": [str],
        "total_records_processed": int,
        "documents_extracted": int,
        "relationships_created": int,
        "duplicates_filtered": int,
        "validation_errors": int,
        "data_quality_score": float
      },
      "key_statistics": {
        "emisores_created": int,
        "informes_processed": int,
        "macroeconomic_data_points": int,
        "contracts_identified": int,
        "daily_movements": int
      }
    }

  context: [extract_task]
  output_file: 'output/try_1/structured_data_output.txt'

vectorize_task: 
  description: >
    Transform documents and extracted content into high-quality vector embeddings for semantic search across 3 Pinecone indices.
    Process both PDF documents and structured content through a comprehensive 4-stage vectorization pipeline.
    
    Sequential Tool Workflow:
    
    1. **chunk_document** - Intelligent Text Chunking
       - Input: Extracted document text and structured data from 'structured_data_output.txt'
       - Chunk size: 1200 tokens (optimized for Gemini embeddings)
       - Overlap: 200 tokens to preserve semantic continuity
       - Use tiktoken for accurate token counting with fallback to character-based chunking
       - Preserve semantic boundaries at sentence/paragraph breaks
       - Maintain context across chunks with proper overlap management
       - Output: List of text chunks with position metadata
    
    2. **prepare_document_metadata** - Multi-Index Metadata Preparation
       - Input: Chunks from step 1 + structured data relationships
       - Prepare metadata for 3 Pinecone indices:
         * documentos-informes-vector: Full document embeddings
         * dato-macroeconomico-vector: Economic indicator context
         * licitacion-contrato-vector: Contract and tender details
       - Generate unique UUID-based vector IDs
       - Create rich metadata linking to Supabase records
       - Include processing timestamps and content classification
       - Output: Vector-ready data with index-specific metadata
    
    3. **filter_duplicate_vectors** - Vector Deduplication
       - Input: Prepared vector data from step 2
       - Check existing Pinecone indices for duplicate content
       - Use unique field combinations per index for deduplication
       - Filter out existing vectors, keep only new content
       - Report deduplication statistics
       - Output: Final deduplicated vector data ready for embedding
    
    4. **write_vector_data_to_file** - File Persistence [CRITICAL - MUST BE LAST]
       - Input: Final vector data from step 3
       - Write complete vector data to output/try_1/vector_data_output.txt
       - Handles large vector datasets that exceed LLM response limits
       - Ensures data persistence for downstream loading agent
       - Output: Success status with file path and statistics
    
    Index-Specific Processing:
    - **documentos-informes-vector**: Complete document content with structured data references
    - **dato-macroeconomico-vector**: Economic context with metric relationships  
    - **licitacion-contrato-vector**: Contract details with entity associations
    
    Content Sources:
    - Extracted document text from processor agent output (structured_data_output.txt)
    - Structured data records from all 14 Supabase tables
    - Economic indicator descriptions and context
    - Contract and tender detailed information
    
    Quality Requirements:
    - Maintain semantic coherence across chunks
    - Preserve document structure and relationships
    - Ensure proper metadata for each vector index
    - Generate embeddings compatible with Gemini (768 dimensions)
    - Link vectors to corresponding Supabase structured data
  expected_output: >
    File write confirmation with vectorization summary. The vector data has been written to output/try_1/vector_data_output.txt.
    Vectorization Summary:
    {
      "file_write_status": {
        "status": "success",
        "file_path": "output/try_1/vector_data_output.txt",
        "total_vectors_written": int,
        "indices_written": [str],
        "file_size_kb": float
      },
      "vectorization_statistics": {
        "total_chunks_created": int,
        "pdf_documents_processed": int,
        "excel_documents_processed": int,
        "duplicates_filtered": int,
        "indices_populated": [str]
      },
      "index_breakdown": {
        "documentos-informes-vector": {
          "vectors_created": int,
          "average_chunk_size": int
        },
        "dato-macroeconomico-vector": {
          "vectors_created": int,
          "economic_indicators": int
        },
        "licitacion-contrato-vector": {
          "vectors_created": int,
          "contracts_processed": int
        }
      }
    }
  context: [process_task, extract_task]
  output_file: 'output/try_1/vector_data_output.txt'


load_task:
  description: >
    Execute production database loading operations for both structured data (Supabase) and vector data (Pinecone).
    Load processed data through a comprehensive 4-stage pipeline with validation, monitoring, and status reporting.
    
    Sequential Tool Workflow:
    
    1. **validate_data_before_loading** - Pre-Loading Validation
       - Input: Structured data from 'structured_data_output.txt' + vector data from 'vector_data_output.txt'
       - Validate data integrity before database operations
       - Check Supabase table schemas and constraints
       - Validate Pinecone vector dimensions and metadata formats
       - Verify foreign key relationships and data consistency
       - Confirm all required fields are populated
       - Output: Validation report + loading-ready data
    
    2. **load_data_to_supabase** - Structured Data Loading
       - Input: Validated structured data from step 1
       - Load data to all 14 Supabase tables in proper sequence:
         * Master tables first: Categoria_Emisor, Moneda, Frecuencia, etc.
         * Main entities: Emisores, Instrumento, Tipo_Informe, etc.  
         * Transaction data: Informe_General, Resumen_Informe_Financiero
         * Operational data: Movimiento_Diario_Bolsa, Dato_Macroeconomico, Licitacion_Contrato
       - Use optimized batch sizes (50 records per batch)
       - Handle foreign key constraints and relationships
       - Report loading statistics and any errors
       - Output: Supabase loading results + record counts
    
    3. **load_vectors_to_pinecone** - Vector Data Loading
       - Input: Validated vector data from step 1
       - Generate Gemini embeddings for all text chunks
       - Load vectors to 3 Pinecone indices:
         * documentos-informes-vector: Document content embeddings
         * dato-macroeconomico-vector: Economic indicator context  
         * licitacion-contrato-vector: Contract and tender details
       - Use optimized batch sizes (20 vectors per batch)
       - Include rich metadata for semantic search
       - Handle embedding generation errors gracefully
       - Output: Pinecone loading results + vector counts
    
    4. **check_loading_status** - Comprehensive Status Verification
       - Input: Loading results from steps 2 and 3
       - Verify data integrity in both databases
       - Check record counts match expected numbers
       - Validate foreign key relationships in Supabase  
       - Confirm vector embeddings in Pinecone indices
       - Generate comprehensive loading report
       - Output: Complete ETL pipeline status report
    
    Database Operations:
    - **Supabase**: 14 tables with proper constraint handling
    - **Pinecone**: 3 indices with 768-dimension Gemini embeddings
    - **Batch Processing**: Optimized for performance and reliability
    - **Error Handling**: Graceful failure recovery and detailed reporting
    
    Quality Assurance:
    - Pre-loading validation prevents data corruption
    - Proper loading sequence maintains referential integrity
    - Comprehensive status checks ensure data quality
    - Detailed reporting enables troubleshooting
    
    Production Requirements:
    - All environment variables must be configured
    - Database connections must be established
    - API keys for Supabase, Pinecone, and Gemini required
    - Error handling for network issues and API limits
  expected_output: >
    Comprehensive ETL pipeline completion report:
    {
      "loading_results": {
        "supabase_results": {
          "tables_loaded": [str],
          "total_records_inserted": int,
          "loading_errors": [str],
          "batch_statistics": {"table_name": {"batches": int, "records": int}},
          "foreign_key_validations": {"table_name": "status"}
        },
        "pinecone_results": {
          "indices_loaded": [str], 
          "total_vectors_inserted": int,
          "embedding_errors": [str],
          "batch_statistics": {"index_name": {"batches": int, "vectors": int}},
          "embedding_generation_time": float
        }
      },
      "data_integrity_report": {
        "supabase_validation": {
          "record_counts": {"table_name": int},
          "constraint_violations": [str],
          "orphaned_records": [str]
        },
        "pinecone_validation": {
          "vector_counts": {"index_name": int},
          "embedding_dimension_check": "768_confirmed",
          "metadata_completeness": {"index_name": float}
        },
        "cross_database_links": {
          "structured_to_vector_mapping": int,
          "unmapped_records": [str]
        }
      },
      "pipeline_summary": {
        "total_execution_time": float,
        "extraction_records": int,
        "processing_success_rate": float,
        "vectorization_success_rate": float,
        "loading_success_rate": float,
        "overall_pipeline_status": "SUCCESS|PARTIAL|FAILED"
      }
    }

  context: [process_task, vectorize_task]
  output_file: 'output/try_1/loading_results_output.txt'
