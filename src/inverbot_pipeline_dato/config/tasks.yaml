extract_task:
  description: >
    ðŸš¨ CRITICAL TASK: Execute ALL 10 scraper tools in this EXACT sequence. DO NOT SKIP ANY TOOLS.
    
    MANDATORY EXECUTION ORDER (ALL 10 TOOLS REQUIRED):
    
    1. **BVA Sources** (4 tools) - Execute ALL:
       ðŸ”§ Call "BVA Emisores Scraper" tool
       ðŸ”§ Call "BVA Daily Reports Scraper" tool  
       ðŸ”§ Call "BVA Monthly Reports Scraper" tool
       ðŸ”§ Call "BVA Annual Reports Scraper" tool
    
    2. **Government Data** (3 tools) - Execute ALL:
       ðŸ”§ Call "Paraguay Open Data Scraper" tool
       ðŸ”§ Call "INE Statistics Scraper" tool
       ðŸ”§ Call "INE Social Publications Scraper" tool
    
    3. **Contracts & Investment** (3 tools) - Execute ALL:
       ðŸ”§ Call "Public Contracts Scraper" tool
       ðŸ”§ Call "DNIT Investment Data Scraper" tool
       ðŸ”§ Call "DNIT Financial Reports Scraper" tool
    
    ðŸŽ¯ EXECUTION REQUIREMENTS:
    - You MUST call ALL 10 tools in the above order
    - Each tool call MUST use test_mode=True parameter
    - Continue execution even if some tools fail or return errors
    - Each tool automatically writes its data to raw_extraction_output.txt
    - Final output must contain data from ALL 10 sources
    
    ðŸ’¡ TOOL BEHAVIOR:
    - Each scraper tool has built-in API integration (DNCP v3, BVA JSON APIs)
    - Automatic fallback to realistic mock data if APIs fail
    - Tools write directly to output/try_1/raw_extraction_output.txt
    - Comprehensive error handling prevents pipeline failure
  expected_output: >
    ðŸ“‹ COMPLETION CONFIRMATION: After executing all 10 tools successfully:
    
    "âœ… EXTRACTION COMPLETE - All 10 Paraguayan financial data sources processed:
    
    ðŸ¦ BVA Sources: 4/4 complete
    - Emisores content extracted with company ratings
    - Daily market data with real API feeds
    - Monthly volumes (1.52T GS confirmed)
    - Annual reports with PDF document links
    
    ðŸ›ï¸ Government Sources: 3/3 complete  
    - Paraguay Open Data portal content
    - INE macroeconomic indicators
    - INE social statistics
    
    ðŸ“Š Contracts & Investment: 3/3 complete
    - DNCP contracts via API v3 (USD 2.84B total)
    - DNIT investment opportunities
    - DNIT financial reports (723 documents)
    
    ðŸ“„ Output: raw_extraction_output.txt contains complete JSON structure with all 10 data sources populated.
    Total content extracted: [X] characters, [Y] links, [Z] documents.
    
    Pipeline ready for processing phase."
    
    Raw content structure organized by data source with comprehensive content capture:
    {
      "bva_sources": {
        "emisores_content": {"page_content": str, "links": [str], "documents": [str], "metadata": dict},
        "daily_content": {"page_content": str, "links": [str], "documents": [str], "metadata": dict},
        "monthly_content": {"page_content": str, "links": [str], "documents": [str], "metadata": dict},
        "annual_content": {"page_content": str, "links": [str], "documents": [str], "metadata": dict}
      },
      "government_sources": {
        "datos_gov_content": {"page_content": str, "links": [str], "documents": [str], "metadata": dict},
        "ine_main_content": {"page_content": str, "links": [str], "documents": [str], "metadata": dict},
        "ine_social_content": {"page_content": str, "links": [str], "documents": [str], "metadata": dict}
      },
      "contracts_investment_sources": {
        "contracts_content": {"page_content": str, "links": [str], "documents": [str], "metadata": dict},
        "dnit_investment_content": {"page_content": str, "links": [str], "documents": [str], "metadata": dict},
        "dnit_financial_content": {"page_content": str, "links": [str], "documents": [str], "metadata": dict}
      },
      "extraction_summary": {
        "total_sources_processed": int, "content_length_total": int, "links_found_total": int, "documents_found_total": int
      }
    }

  output_file: 'output/try_1/raw_extraction_output.txt'

process_task:
  description: >
    UPDATED PIPELINE ARCHITECTURE: Process raw extracted content through a streamlined 4-stage pipeline 
    using new file-based tools to create production-ready structured data for all 14 Supabase tables.
    
    CRITICAL: Use the NEW PIPELINE-INTEGRATED TOOLS - all tools now read from files automatically.
    
    Sequential Tool Workflow:
    
    1. **Extract Structured Data from Raw Content** - Intelligent Content Structuring
       - TOOL: extract_structured_data_from_raw (NEW: reads from raw_extraction_output.txt automatically)
       - Reads raw content from extractor agent's output file
       - Intelligently extracts structured data from unstructured text
       - Processes BVA financial data (bond emissions, company ratings, trading volumes)
       - Routes processing based on source (BVA, INE, DNIT, contracts, etc.)
       - Converts raw content into structured format for all 14 database tables
       - Handles malformed JSON and incomplete data gracefully
       - Output: Writes structured_data_output.txt with organized data
    
    2. **Normalize Data Tool** - Data Cleaning & Standardization
       - TOOL: normalize_data (NEW: reads from structured_data_output.txt automatically)
       - Reads structured data from step 1 output file
       - Cleans HTML artifacts, special characters, encoding issues
       - Standardizes date formats (YYYY-MM-DD), number formats (decimal)
       - Normalizes emisor names to lowercase with dashes
       - Handles missing values and data type inconsistencies
       - Output: Writes normalized_data_output.txt with cleaned data
    
    3. **Validate Data Tool** - Schema Validation  
       - TOOL: validate_data (NEW: reads from normalized_data_output.txt automatically)
       - Reads normalized data from step 2 output file
       - Validates against all 14 Supabase table schemas (UPDATED schemas)
       - Checks data types, field lengths, required constraints
       - Validates foreign key references and relationships
       - Separates valid data from invalid entries with error reporting
       - Output: Writes validated_data_output.txt with validation results
    
    4. **Create Entity Relationships Tool** - Relationship Building
       - TOOL: create_entity_relationships (NEW: reads from validated_data_output.txt automatically)
       - Reads validated data from step 3 output file
       - Resolves entity names to proper IDs (emisor names â†’ id_emisor)
       - Creates master entities first (Categoria_Emisor, Moneda, etc.)
       - Establishes foreign key relationships across all tables
       - Handles missing references with appropriate defaults
       - Output: Writes relationships_data_output.txt with complete entity relationships
    
    Quality Checkpoints:
    - Validate data integrity at each stage
    - Report processing statistics and error counts
    - Ensure referential integrity across all relationships
    - Confirm all required fields are populated
    
    Target Schemas: All 14 Supabase tables including Categoria_Emisor, Emisores, Moneda, Frecuencia, 
    Tipo_Informe, Periodo_Informe, Unidad_Medida, Instrumento, Informe_General, 
    Resumen_Informe_Financiero, Dato_Macroeconomico, Movimiento_Diario_Bolsa, Licitacion_Contrato.
  expected_output: >
    File write confirmation with processing summary. The structured data has been written to output/try_1/structured_data_output.txt.
    Processing Summary:
    {
      "file_write_status": {
        "status": "success",
        "file_path": "output/try_1/structured_data_output.txt",
        "total_records_written": int,
        "tables_written": [str],
        "file_size_kb": float
      },
      "processing_summary": {
        "tables_populated": [str],
        "total_records_processed": int,
        "documents_extracted": int,
        "relationships_created": int,
        "duplicates_filtered": int,
        "validation_errors": int,
        "data_quality_score": float
      },
      "key_statistics": {
        "emisores_created": int,
        "informes_processed": int,
        "macroeconomic_data_points": int,
        "contracts_identified": int,
        "daily_movements": int
      }
    }

  context: [extract_task]
  output_file: 'output/try_1/structured_data_output.txt'

vectorize_task: 
  description: >
    UPDATED PIPELINE ARCHITECTURE: Transform structured financial data into high-quality vector embeddings 
    for semantic search across 3 Pinecone indices using new pipeline-integrated tools.
    
    CRITICAL: Use the NEW PIPELINE-INTEGRATED TOOL - automatically reads from structured data files.
    
    Sequential Tool Workflow:
    
    1. **Process Structured Data for Vectorization** - Complete Vector Generation Pipeline
       - TOOL: process_structured_data_for_vectorization (NEW: reads from relationships_data_output.txt automatically)
       - Reads structured data with relationships from processor agent output
       - Creates meaningful vector content for all 3 Pinecone indices:
         * documento_informe_vector: Bond operations, issuer information, financial movements
         * dato_macroeconomico_vector: Economic indicators with context (trading volumes, economic metrics)
         * licitacion_contrato_vector: Contract and tender information
       - Generates descriptive text from structured financial data
       - Creates proper metadata linking to Supabase records
       - Assigns unique UUIDs for each vector
       - Handles all data transformation internally
       - Output: Writes vector_data_output.txt with complete vector-ready data
    
    2. **Write Vector Data to File** - File Persistence [AUTOMATIC]
       - TOOL: write_vector_data_to_file (if needed for additional processing)
       - Handles large vector datasets that exceed LLM response limits
       - Ensures data persistence for downstream loading agent
       - Already integrated into process_structured_data_for_vectorization
       - Output: Confirms vector data file creation
    
    Index-Specific Processing:
    - **documentos-informes-vector**: Complete document content with structured data references
    - **dato-macroeconomico-vector**: Economic context with metric relationships  
    - **licitacion-contrato-vector**: Contract details with entity associations
    
    Content Sources:
    - Extracted document text from processor agent output (structured_data_output.txt)
    - Structured data records from all 14 Supabase tables
    - Economic indicator descriptions and context
    - Contract and tender detailed information
    
    Quality Requirements:
    - Maintain semantic coherence across chunks
    - Preserve document structure and relationships
    - Ensure proper metadata for each vector index
    - Generate embeddings compatible with Gemini (768 dimensions)
    - Link vectors to corresponding Supabase structured data
  expected_output: >
    File write confirmation with vectorization summary. The vector data has been written to output/try_1/vector_data_output.txt.
    Vectorization Summary:
    {
      "file_write_status": {
        "status": "success",
        "file_path": "output/try_1/vector_data_output.txt",
        "total_vectors_written": int,
        "indices_written": [str],
        "file_size_kb": float
      },
      "vectorization_statistics": {
        "total_chunks_created": int,
        "pdf_documents_processed": int,
        "excel_documents_processed": int,
        "duplicates_filtered": int,
        "indices_populated": [str]
      },
      "index_breakdown": {
        "documentos-informes-vector": {
          "vectors_created": int,
          "average_chunk_size": int
        },
        "dato-macroeconomico-vector": {
          "vectors_created": int,
          "economic_indicators": int
        },
        "licitacion-contrato-vector": {
          "vectors_created": int,
          "contracts_processed": int
        }
      }
    }
  context: [process_task, extract_task]
  output_file: 'output/try_1/vector_data_output.txt'


load_task:
  description: >
    ðŸš¨ MANDATORY: You MUST call BOTH loading tools in sequence - Supabase AND Pinecone loading.
    BOTH tools are required for complete pipeline success. DO NOT SKIP ANY TOOL.
    
    UPDATED PIPELINE ARCHITECTURE: Execute database loading operations for both structured data (Supabase) 
    and vector data (Pinecone) using new pipeline-integrated tools that automatically read from files.
    
    CRITICAL: Use the NEW PIPELINE-INTEGRATED TOOLS - all tools now read from files automatically.
    
    MANDATORY Sequential Tool Workflow (BOTH TOOLS REQUIRED):
    
    **PRIMARY OPTION - Use This First:**
    0. **Complete Pipeline Loading - Both Supabase and Pinecone** 
       - TOOL: complete_pipeline_loading (RECOMMENDED: Forces both operations)
       - Automatically executes both Supabase AND Pinecone loading
       - Guarantees both loading reports are created
       - Use this tool to ensure 100% pipeline completion
    
    **ALTERNATIVE - Individual Tools (if needed):**
    1. **Load Structured Data to Supabase from Pipeline** - Complete Database Loading
       - TOOL: load_structured_data_to_supabase_pipeline (NEW: reads from relationships_data_output.txt automatically)
       - Reads structured data with relationships from processor agent output
       - Loads data to all 14 Supabase tables with proper sequence:
         * Master tables: Categoria_Emisor, Moneda, Frecuencia, Unidad_Medida, etc.
         * Entity tables: Emisores, Instrumento, Tipo_Informe, etc.
         * Transaction tables: Movimiento_Diario_Bolsa, Dato_Macroeconomico, etc.
       - Handles foreign key constraints and relationships automatically
       - Uses corrected schema definitions (nombre_instrumento not simbolo_instrumento)
       - Generates deterministic IDs from unique field combinations
       - In TEST_MODE: Saves data as JSON files instead of database loading
       - Output: Writes supabase_loading_report.json with comprehensive results
    
    2. **Load Vectors to Pinecone from Pipeline** - Complete Vector Loading
       - TOOL: load_vectors_to_pinecone_pipeline (NEW: reads from vector_data_output.txt automatically)
       - Reads vector data from vectorization agent output
       - Loads vectors to 3 Pinecone indices:
         * documento_informe_vector: Financial operations and issuer data
         * dato_macroeconomico_vector: Economic indicators and trading volumes
         * licitacion_contrato_vector: Contract and tender information
       - In TEST_MODE: Generates mock 768-dimensional embeddings deterministically
       - Creates proper metadata for semantic search
       - Handles batch processing automatically
       - Output: Writes pinecone_loading_report.json with comprehensive results
    
    3. **Check Loading Status** - Status Verification (if needed)
       - TOOL: check_loading_status (for additional verification if needed)
       - Verifies loading results from both tools
       - Checks file outputs and data integrity
       - Generates comprehensive ETL pipeline status report
       - Output: Complete pipeline status verification
    
    Database Operations:
    - **Supabase**: 14 tables with proper constraint handling
    - **Pinecone**: 3 indices with 768-dimension Gemini embeddings
    - **Batch Processing**: Optimized for performance and reliability
    - **Error Handling**: Graceful failure recovery and detailed reporting
    
    Quality Assurance:
    - Pre-loading validation prevents data corruption
    - Proper loading sequence maintains referential integrity
    - Comprehensive status checks ensure data quality
    - Detailed reporting enables troubleshooting
    
    Production Requirements:
    - All environment variables must be configured
    - Database connections must be established
    - API keys for Supabase, Pinecone, and Gemini required
    - Error handling for network issues and API limits
  expected_output: >
    ðŸš¨ MANDATORY VALIDATION: You MUST call both loading tools and generate BOTH reports:
    âœ… load_structured_data_to_supabase_pipeline â†’ supabase_loading_report.json
    âœ… load_vectors_to_pinecone_pipeline â†’ pinecone_loading_report.json
    
    BOTH files must be created - no exceptions!
    
    Comprehensive ETL pipeline completion report:
    {
      "loading_results": {
        "supabase_results": {
          "tables_loaded": [str],
          "total_records_inserted": int,
          "loading_errors": [str],
          "batch_statistics": {"table_name": {"batches": int, "records": int}},
          "foreign_key_validations": {"table_name": "status"}
        },
        "pinecone_results": {
          "indices_loaded": [str], 
          "total_vectors_inserted": int,
          "embedding_errors": [str],
          "batch_statistics": {"index_name": {"batches": int, "vectors": int}},
          "embedding_generation_time": float
        }
      },
      "data_integrity_report": {
        "supabase_validation": {
          "record_counts": {"table_name": int},
          "constraint_violations": [str],
          "orphaned_records": [str]
        },
        "pinecone_validation": {
          "vector_counts": {"index_name": int},
          "embedding_dimension_check": "768_confirmed",
          "metadata_completeness": {"index_name": float}
        },
        "cross_database_links": {
          "structured_to_vector_mapping": int,
          "unmapped_records": [str]
        }
      },
      "pipeline_summary": {
        "total_execution_time": float,
        "extraction_records": int,
        "processing_success_rate": float,
        "vectorization_success_rate": float,
        "loading_success_rate": float,
        "overall_pipeline_status": "SUCCESS|PARTIAL|FAILED"
      }
    }

  context: [process_task, vectorize_task]
  output_file: 'output/try_1/loading_results_output.txt'
