{"chunks": [{"chunk_id": 0, "text": "This is a sample document.  It contains multiple sentences to demonstrate the chunking process.  We need to break it down into smaller pieces to create embeddings that are suitable for vector databases. Each chunk should be approximately 1200 tokens or less. The chunking process is crucial for efficient vector database operations and semantic search.  This is another sentence.  And another.  And yet another. We'll continue adding sentences until we reach a suitable length for testing purposes. This lengthy text will allow us to effectively test the chunking functionality and ensure that the resulting chunks are appropriately sized for embedding generation and subsequent vector database operations. This is a crucial step in our testing process. We need to make sure that the chunks are of high quality and can be used for semantic search and content relationship mapping in our vector database testing.  More text here to ensure sufficient length for proper testing.  More sentences to reach the desired length.  Additional text for testing the chunking mechanism.  More text. More text. More text.  More text. More text. More text. More text. More text. More text. More text. More text.", "char_count": 1196, "position": 0}, {"chunk_id": 1, "text": "More text More text. More text. More text.  More text.  More text.  More text. More text. More text. More text. More text. More text. More text. More text.  More text.  More text. More text. More text. More text. More text. More text. More text. More text. More text. More text.  More text.  More text. More text. More text. More text. More text. More text. More text. More text. More text. More text. More text.  More text. More text..", "char_count": 437, "position": 1}], "total_chunks": 2, "chunking_report": {"status": "success", "original_length": 1633, "chunks_created": 2}}